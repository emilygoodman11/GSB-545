{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Specs\n",
        "\n",
        "You should compare XGBoost or Gradient Boosting to the results of your previous AdaBoost activity.\n",
        "Based on the visualizations seen at the links above you're probably also thinking that this classification task should not be that difficult. So, a secondary goal of this assignment is to test the effects of the XGBoost (or Gradient Boosting) function arguments on the algorithm's performance.\n",
        "You should explore at least 3 different sets of settings for the function inputs, and you should do your best to find values for these inputs that actually change the results of your modelling. That is, try not to run three different sets of inputs that result in the same performance. The goal here is for you to better understand how to set these input values yourself in the future. Comment on what you discover about these inputs and how the behave.\n",
        "Your submission should be built and written with non-experts as the target audience. All of your code should still be included, but do your best to narrate your work in accessible ways.\n",
        "Again, submit an HTML, ipynb, or Colab link. Be sure to rerun your entire notebook fresh before submitting!\n"
      ],
      "metadata": {
        "id": "bSs-979oNXgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n"
      ],
      "metadata": {
        "id": "xBu4ndXsQOmz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nV4buPGCNNVT"
      },
      "outputs": [],
      "source": [
        "\n",
        "size= pd.read_csv(\"/content/penguins_size.csv\")\n",
        "dataset=size\n",
        "\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "# Optional: reset index after dropping rows\n",
        "dataset = dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradient boosting"
      ],
      "metadata": {
        "id": "oROlud7nQXzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Encode categorical columns\n",
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit_transform(dataset[label])\n",
        "\n",
        "# Define features and target\n",
        "X = dataset.drop(['species'], axis=1)\n",
        "Y = dataset['species']\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "gbc = GradientBoostingClassifier(n_estimators=400, learning_rate=1.0, max_depth=3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gbc.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy is:', accuracy * 100, '%')\n",
        "\n",
        "# Get feature importances\n",
        "importances = gbc.feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop Important Features:\")\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKAY1NCfQEzU",
        "outputId": "63981a03-46d2-47d2-8674-f5c2f2f72604"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy is: 99.00990099009901 %\n",
            "\n",
            "Top Important Features:\n",
            "             Feature    Importance\n",
            "1   culmen_length_mm  4.938379e-01\n",
            "3  flipper_length_mm  3.327490e-01\n",
            "0             island  1.588311e-01\n",
            "2    culmen_depth_mm  1.273897e-02\n",
            "4        body_mass_g  1.842956e-03\n",
            "5                sex  6.166081e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "xg boost"
      ],
      "metadata": {
        "id": "A3TLrJFyQoje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit_transform(dataset[label])\n",
        "\n",
        "# Define features and target\n",
        "X = dataset.drop(['species'], axis=1)\n",
        "Y = dataset['species']\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(n_estimators=400, learning_rate=1.0, max_depth=3, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Train model\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy is:', accuracy * 100, '%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dogCne1LQjFt",
        "outputId": "32c1df8a-c5d8-4e0f-d8cb-3c34f05d6dae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [15:40:52] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy is: 99.00990099009901 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "importances = xgb.feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop Important Features:\")\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY0IkeI5RD81",
        "outputId": "39fe3cbf-06fb-4139-9806-b88b22773233"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top Important Features:\n",
            "             Feature  Importance\n",
            "3  flipper_length_mm    0.564384\n",
            "0             island    0.224471\n",
            "1   culmen_length_mm    0.187995\n",
            "2    culmen_depth_mm    0.015092\n",
            "5                sex    0.004768\n",
            "4        body_mass_g    0.003289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now trying to mess up the functions"
      ],
      "metadata": {
        "id": "ohfedwAgRI68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit_transform(dataset[label])\n",
        "\n",
        "# Define features and target\n",
        "X = dataset.drop(['species'], axis=1)\n",
        "Y = dataset['species']\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(n_estimators=1, learning_rate=10000000, max_depth=1, eval_metric='mlogloss', random_state=4)\n",
        "\n",
        "# Train model\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy is:', accuracy * 100, '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l3TmedBRR83",
        "outputId": "7ecafdfe-4de6-49d4-ca1e-664bb5cdad9d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy is: 94.05940594059405 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for label in dataset.columns:\n",
        "    dataset[label] = LabelEncoder().fit_transform(dataset[label])\n",
        "\n",
        "# Define features and target\n",
        "X = dataset.drop(['species', 'island'], axis=1)\n",
        "Y = dataset['species']\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(n_estimators=10, learning_rate=1000, max_depth=1000, eval_metric='mlogloss', random_state=4)\n",
        "\n",
        "# Train model\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = xgb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Test set accuracy is:', accuracy * 100, '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvNgN9kzS7og",
        "outputId": "1d27dfc9-fea5-4e7b-9d73-647a9a4411d9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy is: 97.02970297029702 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i messed around with max depth, n estimators, and learning rate to optimize model performance and strike a balance between underfitting and overfitting.\n",
        "\n",
        "max_depth controls the complexity of each individual decision tree\n",
        "n_estimators defines how many boosting rounds (trees) the model builds.\n",
        "learning_rate determines the contribution of each tree to the final prediction."
      ],
      "metadata": {
        "id": "Q9dw_PLkTeDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last time using adaboost i got an accuracy of 96%, so both graident(99%) and xgboost(99%) did better than ada boost."
      ],
      "metadata": {
        "id": "dPNHdRZhUNA3"
      }
    }
  ]
}