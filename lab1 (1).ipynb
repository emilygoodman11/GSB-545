{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this lab you'll be doing a bit more of a comprehensive analysis. Despite coming from Kaggle, this will not be a competition. Check out the data source and context here:\n",
        "\n",
        "https://www.kaggle.com/datasets/riinuanslan/sleep-data-from-fitbit-trackerLinks to an external site.\n",
        "\n",
        "It's a relatively simple dataset, but your goal is this:\n",
        "\n",
        "Can we predict Sleep Score (posted by the FitBit app) using the other metrics in the dataset? In other words, is there a formula here that the FitBit app uses to compute Sleep Score that we can reverse-engineer?\n",
        "Two constraints for this assignment:\n",
        "\n",
        "1. Your modeling efforts must involve bagging and stacking in some way. Otherwise, you may try whatever you like.\n",
        "\n",
        "2. You are allowed, even encouraged, to compute and/or gather additional features to use as explanatory variables in your model. For example, you might create a variable for the time they went to sleep (as a measure of how \"early\" they went to bed, or not). There are multiple datasets and you should use all of them, which means you may use the corresponding month for the dataset as a variable as well (or anything related to it).\n",
        "\n",
        "Your submission should be an HTML or .ipynb file of all of your work.\n",
        "\n"
      ],
      "metadata": {
        "id": "50j2pNeMZheb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cleaning the data"
      ],
      "metadata": {
        "id": "Xrdzuzb-4xL-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL4YUcWNZbjy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import StackingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "april=pd.read_csv('/content/April sleep data - Sheet1.csv')\n",
        "december=pd.read_csv('/content/December Sleep data - Sheet1.csv')\n",
        "february=pd.read_csv('/content/February sleep data - Sheet1 (1).csv')\n",
        "january=pd.read_csv('/content/January sleep data - Sheet1.csv')\n",
        "march=pd.read_csv('/content/March sleep data - Sheet1.csv')\n",
        "november=pd.read_csv('/content/November Sleep Data - Sheet1.csv')\n"
      ],
      "metadata": {
        "id": "2Kz8VpBV3KeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "february.rename(columns={\"SLEEP SQORE\": \"SLEEP SCORE\"}, inplace=True)\n",
        "february.rename(columns={\"FEBEUARY\": \"FEBRUARY\"}, inplace=True)\n"
      ],
      "metadata": {
        "id": "0nfT4xuS3kV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "february.rename(columns={\"FEBRUARY\": \"DAYS OF THE WEEK\"}, inplace=True)\n",
        "april.rename(columns={\"APRIL\": \"DAYS OF THE WEEK\"}, inplace=True)\n",
        "december.rename(columns={\"DECEMBER\": \"DAYS OF THE WEEK\"}, inplace=True)\n",
        "january.rename(columns={\"JANUARY\": \"DAYS OF THE WEEK\"}, inplace=True)\n",
        "march.rename(columns={\"MARCH\": \"DAYS OF THE WEEK\"}, inplace=True)\n",
        "november.rename(columns={\"NOVEMBER\": \"DAYS OF THE WEEK\"}, inplace=True)"
      ],
      "metadata": {
        "id": "n9b0GNKz3t8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "april[\"MONTH\"] = \"APRIL\"\n",
        "december[\"MONTH\"] = \"DECEMBER\"\n",
        "february[\"MONTH\"] = \"FEBRUARY\"\n",
        "january[\"MONTH\"] = \"JANUARY\"\n",
        "march[\"MONTH\"] = \"MARCH\"\n",
        "november[\"MONTH\"] = \"NOVEMBER\""
      ],
      "metadata": {
        "id": "Ot-8DNoE30qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "january.rename(columns={\"HEART RATE UNDER RESTING\": \"HEART RATE BELOW RESTING\"}, inplace=True)\n",
        "march.rename(columns={\"HEARTRATE BELOW RESTING\": \"HEART RATE BELOW RESTING\"}, inplace=True)"
      ],
      "metadata": {
        "id": "I32iDF6A3-te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "april = april.dropna()\n",
        "december = december.dropna()\n",
        "february = february.dropna()\n",
        "january = january.dropna()\n",
        "march = march.dropna()\n",
        "november = november.dropna()"
      ],
      "metadata": {
        "id": "YP6vchBc4Se1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sleep_data = pd.concat([november, december, january, february, march, april], ignore_index=True)"
      ],
      "metadata": {
        "id": "aZRt7vhY4ZLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#had chat help with doing this\n",
        "\n",
        "# Split \"SLEEP TIME\" into two separate columns\n",
        "all_sleep_data[[\"SLEEP START\", \"SLEEP END\"]] = all_sleep_data[\"SLEEP TIME\"].str.split(\" - \", expand=True)\n",
        "\n",
        "# Clean and standardize time strings\n",
        "def fix_time_string(s):\n",
        "    if pd.isna(s):\n",
        "        return s\n",
        "    s = s.strip().lower()\n",
        "    s = re.sub(r\"(\\d{1,2})-(\\d{2})(am|pm)\", r\"\\1:\\2\\3\", s)\n",
        "    s = re.sub(r\"[^\\dxapm:]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "all_sleep_data[\"SLEEP START\"] = all_sleep_data[\"SLEEP START\"].apply(fix_time_string)\n",
        "all_sleep_data[\"SLEEP END\"] = all_sleep_data[\"SLEEP END\"].apply(fix_time_string)\n",
        "\n",
        "# Convert times to datetime, then to string format\n",
        "all_sleep_data[\"SLEEP START\"] = pd.to_datetime(all_sleep_data[\"SLEEP START\"], errors=\"coerce\").dt.strftime(\"%H:%M\")\n",
        "all_sleep_data[\"SLEEP END\"] = pd.to_datetime(all_sleep_data[\"SLEEP END\"], errors=\"coerce\").dt.strftime(\"%H:%M\")\n",
        "\n",
        "# Fix time strings to have seconds (e.g., HH:MM:SS)\n",
        "all_sleep_data['HOURS OF SLEEP'] = all_sleep_data['HOURS OF SLEEP'].apply(\n",
        "    lambda x: x if len(x.split(':')) == 3 else f\"{x}:00\"\n",
        ")\n",
        "\n",
        "# Convert sleep duration to float hours\n",
        "all_sleep_data['HOURS OF SLEEP'] = pd.to_timedelta(all_sleep_data['HOURS OF SLEEP']).dt.total_seconds() / 3600\n",
        "\n",
        "# Convert percentage columns to float\n",
        "for col in ['REM SLEEP', 'DEEP SLEEP', 'HEART RATE BELOW RESTING']:\n",
        "    all_sleep_data[col] = all_sleep_data[col].str.replace('%', '', regex=False).astype(float)\n",
        "\n",
        "# Convert sleep start and end times to float hours\n",
        "all_sleep_data['SLEEP START'] = pd.to_datetime(all_sleep_data['SLEEP START'].astype(str), format='%H:%M', errors='coerce')\n",
        "all_sleep_data['SLEEP END'] = pd.to_datetime(all_sleep_data['SLEEP END'].astype(str), format='%H:%M', errors='coerce')\n",
        "\n",
        "all_sleep_data['SLEEP START'] = all_sleep_data['SLEEP START'].dt.hour + all_sleep_data['SLEEP START'].dt.minute / 60\n",
        "all_sleep_data['SLEEP END'] = all_sleep_data['SLEEP END'].dt.hour + all_sleep_data['SLEEP END'].dt.minute / 60\n"
      ],
      "metadata": {
        "id": "rToLPSaM4moh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##models"
      ],
      "metadata": {
        "id": "YPPsKDlz469X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###bagging"
      ],
      "metadata": {
        "id": "bgqk-Mbo49Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Drop rows with missing sleep score\n",
        "all_sleep_data = all_sleep_data.dropna(subset=['SLEEP SCORE'])\n",
        "\n",
        "# Separate features and target\n",
        "X = all_sleep_data.drop(columns=['SLEEP SCORE', 'SLEEP TIME'])  # drop target + any non-feature columns\n",
        "y = all_sleep_data['SLEEP SCORE']\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "1QVHTdlL4s-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
        "print(f\"Average CV R^2 Score (5-fold): {np.mean(cv_scores):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoU0xd0a-u7E",
        "outputId": "fc0b11ab-3cf9-4a93-bddf-38b258188f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 3.77\n",
            "Average CV R^2 Score (5-fold): 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also tried 70, 100, 80, and 90 as the n_estimators but they all produced similar r2 values."
      ],
      "metadata": {
        "id": "yFhzA9hQ7N_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stacking"
      ],
      "metadata": {
        "id": "QlTmzO_SHI1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingRegressor, BaggingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from numpy import mean, std\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Split your data if not already done\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the stacking model with bagging included\n",
        "def get_stacking():\n",
        "    level0 = list()\n",
        "    level0.append(('knn', KNeighborsRegressor()))\n",
        "    level0.append(('cart', DecisionTreeRegressor()))\n",
        "    level0.append(('svm', SVR()))\n",
        "\n",
        "    # Add Bagging Regressor as a base model\n",
        "    bagging = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "    level0.append(('bagging', bagging))\n",
        "\n",
        "    level1 = LinearRegression()\n",
        "    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
        "    return model\n",
        "\n",
        "# Get a list of models to evaluate\n",
        "def get_models():\n",
        "    models = dict()\n",
        "    models['knn'] = KNeighborsRegressor()\n",
        "    models['cart'] = DecisionTreeRegressor()\n",
        "    models['svm'] = SVR()\n",
        "    models['bagging'] = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "    models['stacking'] = get_stacking()\n",
        "    return models\n",
        "\n",
        "# Evaluate a given model using cross-validation\n",
        "def evaluate_model(model, X, y):\n",
        "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "    scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
        "    return scores\n",
        "\n",
        "\n",
        "models = get_models()\n",
        "results, names = list(), list()\n",
        "avg_predictions = dict()\n",
        "\n",
        "print(\"Cross-Validation Results:\")\n",
        "for name, model in models.items():\n",
        "    scores = evaluate_model(model, X_train, y_train)\n",
        "    results.append(scores)\n",
        "    names.append(name)\n",
        "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "\n",
        "    # Fit model and make predictions\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    avg_predictions[name] = np.mean(preds)  # Average prediction for that model\n",
        "\n",
        "# Display average predicted score for each model\n",
        "avg_pred_df = pd.DataFrame(list(avg_predictions.items()), columns=['Model', 'Average Predicted Sleep Score'])\n",
        "\n",
        "print(\"\\nAverage Prediction per Model:\")\n",
        "print(avg_pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNVxpphM_fst",
        "outputId": "20cda9d9-7820-48ef-826d-6c7c0902c3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Results:\n",
            ">knn -1.158 (1.303)\n",
            ">cart 0.446 (0.302)\n",
            ">svm -0.027 (0.077)\n",
            ">bagging 0.659 (0.248)\n",
            ">stacking 0.595 (0.268)\n",
            "\n",
            "Average Prediction per Model:\n",
            "      Model  Average Predicted Sleep Score\n",
            "0       knn                      80.400000\n",
            "1      cart                      84.111111\n",
            "2       svm                      85.443818\n",
            "3   bagging                      84.752222\n",
            "4  stacking                      84.347556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bagging model has the best r^2 of all the models, meaning that its prediction of the average sleep score is the closest to the actual average."
      ],
      "metadata": {
        "id": "LxELQ0a8B7us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I had help from chat making the loops for the stacking code"
      ],
      "metadata": {
        "id": "-MOrYZFNGj-M"
      }
    }
  ]
}